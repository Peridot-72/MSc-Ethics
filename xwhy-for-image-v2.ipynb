{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2557668,"sourceType":"datasetVersion","datasetId":1551831},{"sourceId":2561699,"sourceType":"datasetVersion","datasetId":1554601},{"sourceId":2561817,"sourceType":"datasetVersion","datasetId":1554683}],"dockerImageVersionId":30120,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"top\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Image Explainability using XWhy</b></div>\n\n<a id=\"1.2\"></a>\n<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #155D07; background-color: #ffffff;\"><b>Comparing</b> Image Exlainability of XWhy with LIME and BayLIME</h2>","metadata":{}},{"cell_type":"markdown","source":"<p style=\"text-align: justify;\">With the ever-increasing complexity of machine learning models, explainability has become an indispensable element of effective data analysis. Sophisticated algorithms like deep learning can offer tremendous predictive power, but their \"black box\" nature can obscure understanding and limit their practical application. Hence, the demand for tools and techniques that elucidate the decision-making processes of these models has grown significantly. Among the tools that have emerged to tackle this issue, model-agnostic approaches like Local Interpretable Model-Agnostic Explanations (LIME) and Bayesian Local Interpretable Model-Agnostic Explanations (BayLIME) have made substantial contributions. They provide localized explanations of predictions, offering insights into how and why a specific prediction was made, thus boosting the transparency and interpretability of complex models.</p>\n\n<p style=\"text-align: justify;\">However, as the field of explainability continues to evolve, the quest for superior methods remains unquenched. Our paper introduces a novel approach, X-Why, designed to enhance the explainability of machine learning models beyond the capabilities of existing methods utilizing statistical distance measures like Wasserstein. X-Why aims to improve the interpretability gap left by traditional methods like LIME and BayLIME, taking the science of model explainability a step further. The intent is to provide more comprehensive, intuitive, and robust interpretations of predictions, thus enabling more informed decision-making in software systems. This study aims to present a comparative analysis of X-Why with existing model-agnostic approaches to demonstrate its potential benefits and improvements over other techniques in the field of explainable artificial intelligence.</p>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"top\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:35px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Table of content</b></div>\n\n<div style=\"background-color:aliceblue; padding:30px; font-size:15px;color:#034914\">\n    \n* [Import Libraries](#lib)\n* [Step-by-Step Implementation of LIME](#step_LIME) \n* [Defining LIME Function](#LIME)\n* [Defining XWhy Function with Wasserstein Distance](#xwhy_wd)\n* [Comparing XWhy with LIME](#comp_LIME)\n* [Comparing XWhy with BayLIME](#comp_BayLIME)\n* [Defining XWhy Function with Anderson Darling Distance](#xwhy_add)\n* [Defining XWhy Function with Kolmogorov Smirnov Distance](#xwhy_ksd)\n* [Comparing XWhy with SHAP (Under Construction :D)](#comp_SHAP)\n* [References](#ref)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Import libraries</b></div>","metadata":{}},{"cell_type":"code","source":"#%tensorflow_version 1.x\nimport numpy as np\nimport keras\nfrom keras.applications.imagenet_utils import decode_predictions\nimport skimage.io \nimport skimage.segmentation\nfrom skimage.segmentation import felzenszwalb, slic, quickshift, watershed\nimport copy\nimport sklearn\nimport sklearn.metrics\nfrom sklearn.linear_model import LinearRegression\nimport warnings\n\nimport matplotlib.pyplot as plt\n\nprint('Notebook running: keras ', keras.__version__)\nnp.random.seed(222)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:39.481884Z","iopub.execute_input":"2023-05-26T00:45:39.48233Z","iopub.status.idle":"2023-05-26T00:45:47.335173Z","shell.execute_reply.started":"2023-05-26T00:45:39.482231Z","shell.execute_reply":"2023-05-26T00:45:47.334105Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"step_LIME\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Step-by-step Implementation of LIME</b></div>","metadata":{}},{"cell_type":"markdown","source":"The first step is to import a model that we would like I) to use for image classification and II) to test for explainabiity.","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore') \ninceptionV3_model = keras.applications.inception_v3.InceptionV3() #Load pretrained model\n#inceptionV3_model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:47.336887Z","iopub.execute_input":"2023-05-26T00:45:47.337298Z","iopub.status.idle":"2023-05-26T00:45:53.842185Z","shell.execute_reply.started":"2023-05-26T00:45:47.337257Z","shell.execute_reply":"2023-05-26T00:45:53.84081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The second step is to import the image and resize it for to be used as an input for the model.","metadata":{}},{"cell_type":"code","source":"Xi = skimage.io.imread(\"https://arteagac.github.io/blog/lime_image/img/cat-and-dog.jpg\")\nXi = skimage.transform.resize(Xi, (299,299)) \nXi = (Xi - 0.5)*2 #Inception pre-processing\nskimage.io.imshow(Xi/2+0.5) # Show image before inception preprocessing","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:53.844308Z","iopub.execute_input":"2023-05-26T00:45:53.844709Z","iopub.status.idle":"2023-05-26T00:45:54.622197Z","shell.execute_reply.started":"2023-05-26T00:45:53.844668Z","shell.execute_reply":"2023-05-26T00:45:54.621384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(222)\npreds = inceptionV3_model.predict(Xi[np.newaxis,:,:,:])\ndecode_predictions(preds)[0] #Top 5 classes","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:54.623759Z","iopub.execute_input":"2023-05-26T00:45:54.624029Z","iopub.status.idle":"2023-05-26T00:45:56.463463Z","shell.execute_reply.started":"2023-05-26T00:45:54.624002Z","shell.execute_reply":"2023-05-26T00:45:56.462523Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_pred_classes = preds[0].argsort()[-5:][::-1]\ntop_pred_classes                #Index of top 5 classes","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:56.464571Z","iopub.execute_input":"2023-05-26T00:45:56.464847Z","iopub.status.idle":"2023-05-26T00:45:56.472585Z","shell.execute_reply.started":"2023-05-26T00:45:56.46482Z","shell.execute_reply":"2023-05-26T00:45:56.471336Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"superpixels = skimage.segmentation.quickshift(Xi, kernel_size=4,max_dist=200, ratio=0.2)\nnum_superpixels = np.unique(superpixels).shape[0]\nnum_superpixels","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:56.474294Z","iopub.execute_input":"2023-05-26T00:45:56.474652Z","iopub.status.idle":"2023-05-26T00:45:58.148922Z","shell.execute_reply.started":"2023-05-26T00:45:56.474605Z","shell.execute_reply":"2023-05-26T00:45:58.147968Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Xi.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.150194Z","iopub.execute_input":"2023-05-26T00:45:58.150488Z","iopub.status.idle":"2023-05-26T00:45:58.156906Z","shell.execute_reply.started":"2023-05-26T00:45:58.150459Z","shell.execute_reply":"2023-05-26T00:45:58.155822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.unique(superpixels)","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.160373Z","iopub.execute_input":"2023-05-26T00:45:58.160803Z","iopub.status.idle":"2023-05-26T00:45:58.170956Z","shell.execute_reply.started":"2023-05-26T00:45:58.160762Z","shell.execute_reply":"2023-05-26T00:45:58.169825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skimage.io.imshow(skimage.segmentation.mark_boundaries(Xi/2+0.5, superpixels))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.173102Z","iopub.execute_input":"2023-05-26T00:45:58.17432Z","iopub.status.idle":"2023-05-26T00:45:58.43614Z","shell.execute_reply.started":"2023-05-26T00:45:58.174283Z","shell.execute_reply":"2023-05-26T00:45:58.435103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_perturb = 150\nperturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\nperturbations[0] #Show example of perturbation","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.437374Z","iopub.execute_input":"2023-05-26T00:45:58.437693Z","iopub.status.idle":"2023-05-26T00:45:58.444125Z","shell.execute_reply.started":"2023-05-26T00:45:58.437662Z","shell.execute_reply":"2023-05-26T00:45:58.443207Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perturb_image(img,perturbation,segments):\n    active_pixels = np.where(perturbation == 1)[0]\n    mask = np.zeros(segments.shape)\n    for active in active_pixels:\n          mask[segments == active] = 1 \n    perturbed_image = copy.deepcopy(img)\n    perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n    return perturbed_image","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.445262Z","iopub.execute_input":"2023-05-26T00:45:58.445536Z","iopub.status.idle":"2023-05-26T00:45:58.455856Z","shell.execute_reply.started":"2023-05-26T00:45:58.445509Z","shell.execute_reply":"2023-05-26T00:45:58.454924Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"skimage.io.imshow(perturb_image(Xi/2+0.5,perturbations[0],superpixels))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.457168Z","iopub.execute_input":"2023-05-26T00:45:58.457465Z","iopub.status.idle":"2023-05-26T00:45:58.699135Z","shell.execute_reply.started":"2023-05-26T00:45:58.457437Z","shell.execute_reply":"2023-05-26T00:45:58.698327Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = []\nfor pert in perturbations:\n    perturbed_img = perturb_image(Xi,pert,superpixels)\n    pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n    predictions.append(pred)\n\npredictions = np.array(predictions)\npredictions.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:45:58.700298Z","iopub.execute_input":"2023-05-26T00:45:58.700735Z","iopub.status.idle":"2023-05-26T00:46:21.450712Z","shell.execute_reply.started":"2023-05-26T00:45:58.700693Z","shell.execute_reply":"2023-05-26T00:46:21.449721Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \ndistances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\ndistances.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:21.451876Z","iopub.execute_input":"2023-05-26T00:46:21.452417Z","iopub.status.idle":"2023-05-26T00:46:21.464994Z","shell.execute_reply.started":"2023-05-26T00:46:21.452381Z","shell.execute_reply":"2023-05-26T00:46:21.463661Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kernel_width = 0.25\nweights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function\nweights.shape","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:21.470101Z","iopub.execute_input":"2023-05-26T00:46:21.470577Z","iopub.status.idle":"2023-05-26T00:46:21.80091Z","shell.execute_reply.started":"2023-05-26T00:46:21.470533Z","shell.execute_reply":"2023-05-26T00:46:21.80009Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_to_explain = top_pred_classes[0]\nsimpler_model = LinearRegression()\nsimpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\ncoeff = simpler_model.coef_[0]\ncoeff","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:21.802153Z","iopub.execute_input":"2023-05-26T00:46:21.802485Z","iopub.status.idle":"2023-05-26T00:46:21.863836Z","shell.execute_reply.started":"2023-05-26T00:46:21.802454Z","shell.execute_reply":"2023-05-26T00:46:21.862725Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_top_features = 4\ntop_features = np.argsort(coeff)[-num_top_features:] \ntop_features","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:21.865207Z","iopub.execute_input":"2023-05-26T00:46:21.865801Z","iopub.status.idle":"2023-05-26T00:46:21.873235Z","shell.execute_reply.started":"2023-05-26T00:46:21.865744Z","shell.execute_reply":"2023-05-26T00:46:21.872364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mask = np.zeros(num_superpixels) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels) )","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:21.874417Z","iopub.execute_input":"2023-05-26T00:46:21.874909Z","iopub.status.idle":"2023-05-26T00:46:22.113535Z","shell.execute_reply.started":"2023-05-26T00:46:21.874871Z","shell.execute_reply":"2023-05-26T00:46:22.11246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"LIME\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Defining LIME Function</b></div>\nHere the LIME function is defined as \"xwhy_image\".","metadata":{}},{"cell_type":"code","source":"def xwhy_image(X_input, model, num_perturb = 150, kernel_width = 0.25):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=4, max_dist=200, ratio=0.2)\n    num_superpixels = np.unique(superpixels).shape[0]\n    perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n\n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n    distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n    \n    weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff = simpler_model.coef_[0]\n        \n    return coeff, perturbations","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:22.114925Z","iopub.execute_input":"2023-05-26T00:46:22.115268Z","iopub.status.idle":"2023-05-26T00:46:22.341451Z","shell.execute_reply.started":"2023-05-26T00:46:22.115227Z","shell.execute_reply":"2023-05-26T00:46:22.339572Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff, perturbations = xwhy_image(Xi, model=inceptionV3_model)\n\nprint(len(coeff))\n\nnum_top_features = 4\ntop_features = np.argsort(coeff)[-num_top_features:] \nmask = np.zeros(num_superpixels) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:22.343701Z","iopub.execute_input":"2023-05-26T00:46:22.344354Z","iopub.status.idle":"2023-05-26T00:46:46.709219Z","shell.execute_reply.started":"2023-05-26T00:46:22.344302Z","shell.execute_reply":"2023-05-26T00:46:46.708425Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"xwhy_wd\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Defining XWhy Function with Wasserstein Distance</b></div>","metadata":{}},{"cell_type":"code","source":"def Wasserstein_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        height = abs(F_CDF-E_CDF)\n        width = XY_Sorted[ii+1] - XY_Sorted[ii]\n        Res = Res + (height ** power) * width;  \n \n    return Res\n\ndef  Wasserstein_Dist_PVal(XX, YY):\n    # Information about Bootstrap: \n    # https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    WD = Wasserstein_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_WD = Wasserstein_Dist(comb[e],comb[f]);\n        if (boost_WD > WD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, WD\n\ndef Wasserstein_Dist_Image(img1, img2):\n    if img1.shape[0] != img2.shape[0] or img1.shape[1] != img2.shape[1]:\n        pritn('input images should have the same size')\n    else:\n        WD = []\n        for ii in range(3):\n            \n            im1 = np.array(img1[:,:,ii].flatten())\n            im2 = np.array(img2[:,:,ii].flatten())\n\n            WD.append(Wasserstein_Dist(im1, im2))\n            \n    return sum(WD)\n  \ndef xwhy_image_wd(X_input, model, num_perturb = 400, kernel_width = 0.25):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=3, max_dist=200, ratio=0.25)\n    num_superpixels = np.unique(superpixels).shape[0]\n    perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n    \n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    WD_dist = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n        WD_dist = Wasserstein_Dist_Image(X_input, perturbed_img)\n        \n\n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n    # distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n    \n    weights = np.sqrt(np.exp(-(WD_dist**2)/kernel_width**2)) #Kernel function\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff = simpler_model.coef_[0]\n        \n    return coeff","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:46.710442Z","iopub.execute_input":"2023-05-26T00:46:46.710888Z","iopub.status.idle":"2023-05-26T00:46:46.731817Z","shell.execute_reply.started":"2023-05-26T00:46:46.710849Z","shell.execute_reply":"2023-05-26T00:46:46.730631Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff = xwhy_image_wd(Xi, model=inceptionV3_model, num_perturb = 200, kernel_width = 0.25)\n\nnum_top_features = 4\ntop_features = np.argsort(coeff)[-num_top_features:] \nmask = np.zeros(num_superpixels) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi/2+0.5,mask,superpixels))","metadata":{"execution":{"iopub.status.busy":"2023-05-26T00:46:46.733437Z","iopub.execute_input":"2023-05-26T00:46:46.73374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"comp_LIME\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Comparing XWhy with LIME</b></div>","metadata":{}},{"cell_type":"code","source":"# Xi_L = skimage.io.imread(\"../input/lime-img-ex/LIME_Img_ex.JPG\")\n# Xi_L = skimage.transform.resize(Xi_L, (299,299)) \n# Xi_L = (Xi_L - 0.5)*2 #Inception pre-processing\n# skimage.io.imshow(Xi_L/2+0.5) # Show image before inception preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preds_L = inceptionV3_model.predict(Xi_L[np.newaxis,:,:,:])\n# decode_predictions(preds_L)[0] #Top 5 classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# top_pred_classes_L = preds_L[0].argsort()[-5:][::-1]\n# top_pred_classes_L               #Index of top 5 classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# superpixels_L = skimage.segmentation.quickshift(Xi_L, kernel_size=7,max_dist=100, ratio=0.2)\n# num_superpixels_L = np.unique(superpixels_L).shape[0]\n\n# print(num_superpixels_L)\n\n# skimage.io.imshow(skimage.segmentation.mark_boundaries(Xi_b/2+0.5, superpixels_L))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# coeff, perturbations = xwhy_image(Xi_L, model=inceptionV3_model)\n\n# num_top_features = 4\n# top_features = np.argsort(coeff)[-num_top_features:] \n# mask = np.zeros(num_superpixels) \n# mask[top_features]= True #Activate top superpixels\n# skimage.io.imshow(perturb_image(Xi_L/2+0.5,mask,superpixels) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# coeff = xwhy_image_wd(Xi_L, model=inceptionV3_model)\n\n# num_top_features = 4\n# top_features = np.argsort(coeff)[-num_top_features:] \n# mask = np.zeros(num_superpixels) \n# mask[top_features]= True #Activate top superpixels\n# skimage.io.imshow(perturb_image(Xi_L/2+0.5,mask,superpixels) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"comp_BayLIME\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Comparing XWhy with BayLIME</b></div>","metadata":{}},{"cell_type":"code","source":"Xi_b = skimage.io.imread(\"/kaggle/input/baylimeimgex5/5.jpg\")\nXi_b = skimage.transform.resize(Xi_b, (299,299)) \nXi_b = (Xi_b - 0.5)*2 #Inception pre-processing\nskimage.io.imshow(Xi_b/2+0.5) # Show image before inception preprocessing","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds_b = inceptionV3_model.predict(Xi_b[np.newaxis,:,:,:])\ndecode_predictions(preds_b)[0] #Top 5 classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_pred_classes_b = preds_b[0].argsort()[-5:][::-1]\ntop_pred_classes_b               #Index of top 5 classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"superpixels_b = skimage.segmentation.quickshift(Xi_b, kernel_size=7,max_dist=100, ratio=0.2)\nnum_superpixels_b = np.unique(superpixels_b).shape[0]\n\nprint(num_superpixels_b)\n\nskimage.io.imshow(skimage.segmentation.mark_boundaries(Xi_b/2+0.5, superpixels_b))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# superpixels = skimage.segmentation.quickshift(X_input, kernel_size=3, max_dist=200, ratio=0.25)\n# num_superpixels = np.unique(superpixels).shape[0]\nperturbations_b = np.random.binomial(1, 0.5, size=(200, num_superpixels_b))\n\n\ndef perturb_image(img,perturbation,segments):\n    active_pixels = np.where(perturbation == 1)[0]\n    mask = np.zeros(segments.shape)\n    for active in active_pixels:\n          mask[segments == active] = 1 \n    perturbed_image = copy.deepcopy(img)\n    perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n    return perturbed_image\n\npredictions = []\nWD_dist = []\nfor pert in perturbations_b:\n    perturbed_img = perturb_image(Xi_b,pert,superpixels_b)\n    pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n    predictions.append(pred)\n    WD_dist = Wasserstein_Dist_Image(Xi_b, perturbed_img)\n\npredictions = np.array(predictions)\n\noriginal_image = np.ones(num_superpixels_b)[np.newaxis,:] #Perturbation with all superpixels enabled \n# distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n\nweights = np.sqrt(np.exp(-(WD_dist**2)/kernel_width**2)) #Kernel function\n\nclass_to_explain = top_pred_classes_b[0]\nsimpler_model_b = LinearRegression()\nsimpler_model_b.fit(X=perturbations_b, y=predictions[:,:,class_to_explain], sample_weight=weights)\ncoeff_b = simpler_model_b.coef_[0]\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_b)[-num_top_features:] \nmask = np.zeros(num_superpixels_b) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_b) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_heat_mask(superpixels_b, coeff):\n    # Initialize an empty image\n    heat_mask = np.zeros_like(superpixels_b, dtype=float)\n\n    # Iterate over the unique labels of the superpixels\n    for idx, label in enumerate(np.unique(superpixels_b)):\n        # Set the pixels of the current superpixel to its corresponding coefficient\n        heat_mask[superpixels_b == label] = coeff[idx]\n    \n    return heat_mask\n\nheat_mask_wd = create_heat_mask(superpixels_b, coeff_b)\n\nplt.imshow(heat_mask_wd, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - WD')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"xwhy_add\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Defining XWhy Function with Anderson Darling Distance</b></div>","metadata":{}},{"cell_type":"code","source":"def Anderson_Darling_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    G_CDF = 0\n    height = 0\n    SD = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        G_CDF = G_CDF + 1/n\n        SD = (n * G_CDF * (1-G_CDF))**0.5\n        height = abs(F_CDF - E_CDF)\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: \n            if SD>0: \n                Res = Res + (height/SD)**power\n\n    AD_Dist = Res\n    \n    return AD_Dist\n\ndef  Anderson_Darling_Dist_PVal(XX, YY):\n    # Information about Bootstrap Method: \n    # https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    AD = Anderson_Darling_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_AD = Anderson_Darling_Dist(comb[e],comb[f]);\n        if (boost_AD > AD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, AD\n\ndef Anderson_Darling_Dist_Image(img1, img2):\n    flag = 0\n    if img1.shape[0] != img2.shape[0] or img1.shape[1] != img2.shape[1]:\n        pritn('input images should have the same size')\n    else:\n        ADD = []\n        for ii in range(3):\n            \n            im1 = np.array(img1[:,:,ii].flatten())\n            im2 = np.array(img2[:,:,ii].flatten())\n            \n            AD = Anderson_Darling_Dist(im1, im2)\n            \n            if AD == 0:\n                flag = 1\n            else:\n                ADD.append(AD)\n            \n    return sum(ADD)\n\ndef xwhy_image_ad(X_input, model, perturbations=perturbations_b, num_perturb = 150, kernel_width = 0.9, top_pred_classes= top_pred_classes_b):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=7,max_dist=100, ratio=0.2)\n    num_superpixels = np.unique(superpixels).shape[0]\n    #perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n    \n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    AD_dist = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n        AD_dist = Anderson_Darling_Dist_Image(X_input, perturbed_img)+0.01\n        \n\n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n    # distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n    \n    epsilon = 1e-6  # A small constant\n    weights = np.sqrt(np.exp(-(AD_dist**2)/kernel_width**2)) + epsilon\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff_ad = simpler_model.coef_[0]\n        \n    return coeff_ad","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff_ad = xwhy_image_ad(Xi_b, model=inceptionV3_model)\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_ad)[-num_top_features:] \nmask = np.zeros(num_superpixels_b) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_b) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_ad = create_heat_mask(superpixels_b, coeff_ad)\n\nplt.imshow(heat_mask_ad, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - ADD')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"xwhy_ksd\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Defining XWhy Function with Kolmogorov Smirnov Distance</b></div>\n","metadata":{}},{"cell_type":"code","source":"def Kolmogorov_Smirnov_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0;\n    height = 0;\n    E_CDF = 0;\n    F_CDF = 0;\n    power = 1;\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: height = abs(F_CDF-E_CDF)\n        if height > Res: Res = height\n\n    KS_Dist = Res**power\n    \n    return KS_Dist\n  \ndef  Kolmogorov_Smirnov_Dist_PVal(XX, YY):\n    # Information about Bootstrap Method: \n    # https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    KSD = Kolmogorov_Smirnov_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_KSD = Kolmogorov_Smirnov_Dist(comb[e],comb[f]);\n        if (boost_KSD > KSD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, KSD\n\ndef Kolmogorov_Smirnov_Dist_Image(img1, img2):\n    if img1.shape[0] != img2.shape[0] or img1.shape[1] != img2.shape[1]:\n        pritn('input images should have the same size')\n    else:\n        KSD = []\n        for ii in range(3):\n            \n            im1 = np.array(img1[:,:,ii].flatten())\n            im2 = np.array(img2[:,:,ii].flatten())\n\n            KSD.append(Kolmogorov_Smirnov_Dist(im1, im2))\n            \n    return sum(KSD)\n\ndef xwhy_image_ks(X_input, model, perturbations=perturbations, num_perturb = 150, kernel_width = 0.25, top_pred_classes = top_pred_classes_b):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=7,max_dist=100, ratio=0.2)\n    num_superpixels = np.unique(superpixels).shape[0]\n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    KS_dist = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n        KS_dist = Kolmogorov_Smirnov_Dist_Image(X_input, perturbed_img)\n        \n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n\n    weights = np.sqrt(np.exp(-(KS_dist**2)/kernel_width**2)) #Kernel function\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff_ks = simpler_model.coef_[0]\n        \n    return coeff_ks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff_ks = xwhy_image_ks(Xi_b, model=inceptionV3_model)\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_ks)[-num_top_features:] \nmask = np.zeros(num_superpixels_b) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_b) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_ks = create_heat_mask(superpixels_b, coeff_ks)\n\nplt.imshow(heat_mask_ks, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - KSD')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"xwhy_kd\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Defining XWhy Function with Kuiper Distance</b></div>\n","metadata":{}},{"cell_type":"code","source":"def Kuiper_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    up = 0\n    down = 0\n    Res = 0\n    E_CDF = 0\n    F_CDF = 0\n    height = 0\n    power = 1\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: height = F_CDF-E_CDF\n        if height > up: up = height\n        if height < down: down = height\n\n    K_Dist = abs(down)**power + abs(up)**power\n    \n    return K_Dist\n\ndef  Kuiper_Dist_PVal(XX, YY):\n    # Information about Bootstrap Method: \n    # https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    KD = Kuiper_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_KD = Kuiper_Dist(comb[e],comb[f]);\n        if (boost_KD > KD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, KD\n\ndef Kuiper_Dist_Image(img1, img2):\n    if img1.shape[0] != img2.shape[0] or img1.shape[1] != img2.shape[1]:\n        pritn('input images should have the same size')\n    else:\n        KD = []\n        for ii in range(3):\n            \n            im1 = np.array(img1[:,:,ii].flatten())\n            im2 = np.array(img2[:,:,ii].flatten())\n\n            KD.append(Kuiper_Dist(im1, im2))\n            \n    return sum(KD)\n\ndef xwhy_image_k(X_input, model, perturbations=perturbations, num_perturb = 150, kernel_width = 0.25, top_pred_classes = top_pred_classes_b):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=7,max_dist=100, ratio=0.2)\n    num_superpixels = np.unique(superpixels).shape[0]\n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    K_dist = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n        K_dist = Kuiper_Dist_Image(X_input, perturbed_img)\n        \n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n\n    weights = np.sqrt(np.exp(-(K_dist**2)/kernel_width**2)) #Kernel function\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff_k = simpler_model.coef_[0]\n        \n    return coeff_k","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff_k = xwhy_image_k(Xi_b, model=inceptionV3_model)\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_k)[-num_top_features:] \nmask = np.zeros(num_superpixels_b) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_b) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_k = create_heat_mask(superpixels_b, coeff_k)\n\nplt.imshow(heat_mask_k, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - KD')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"xwhy_ksd\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Defining XWhy Function with Cramer-Von Mises Distance</b></div>\n","metadata":{}},{"cell_type":"code","source":"# Cramer-Von Mises Distance\ndef CVM_Dist(XX, YY):\n  \n    import numpy as np\n    nx = len(XX)\n    ny = len(YY)\n    n = nx + ny\n\n    XY = np.concatenate([XX,YY])\n    X2 = np.concatenate([np.repeat(1/nx, nx), np.repeat(0, ny)])\n    Y2 = np.concatenate([np.repeat(0, nx), np.repeat(1/ny, ny)])\n\n    S_Ind = np.argsort(XY)\n    XY_Sorted = XY[S_Ind]\n    X2_Sorted = X2[S_Ind]\n    Y2_Sorted = Y2[S_Ind]\n\n    Res = 0;\n    E_CDF = 0;\n    F_CDF = 0;\n    power = 1;\n\n    for ii in range(0, n-2):\n        E_CDF = E_CDF + X2_Sorted[ii]\n        F_CDF = F_CDF + Y2_Sorted[ii]\n        height = abs(F_CDF - E_CDF)\n        if XY_Sorted[ii+1] != XY_Sorted[ii]: Res = Res + height**power\n\n    CVM_Dist = Res\n    \n    return CVM_Dist\n\ndef  CVM_Dist_PVal(XX, YY):\n    # Information about Bootstrap Method: \n    # https://towardsdatascience.com/an-introduction-to-the-bootstrap-method-58bcb51b4d60\n    import random\n    nboots = 1000\n    CVMD = CVM_Dist(XX,YY)\n    na = len(XX)\n    nb = len(YY)\n    n = na + nb\n    comb = np.concatenate([XX,YY])\n    reps = 0\n    bigger = 0\n    for ii in range(1, nboots):\n        e = random.sample(range(n), na)\n        f = random.sample(range(n), nb)\n        boost_CVM = CVM_Dist(comb[e],comb[f]);\n        if (boost_CVM > CVMD):\n            bigger = 1 + bigger\n            \n    pVal = bigger/nboots;\n\n    return pVal, CVMD\n\ndef CVM_Dist_Image(img1, img2):\n    if img1.shape[0] != img2.shape[0] or img1.shape[1] != img2.shape[1]:\n        pritn('input images should have the same size')\n    else:\n        CVMD = []\n        for ii in range(3):\n            \n            im1 = np.array(img1[:,:,ii].flatten())\n            im2 = np.array(img2[:,:,ii].flatten())\n\n            CVMD.append(CVM_Dist(im1, im2))\n            \n    return sum(CVMD)\n\ndef xwhy_image_cvm(X_input, model, perturbations=perturbations, num_perturb = 150, kernel_width = 0.85, top_pred_classes = top_pred_classes_b):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=7,max_dist=100, ratio=0.2)\n    num_superpixels = np.unique(superpixels).shape[0]\n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    CVM_dist = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n        CVM_dist = Kuiper_Dist_Image(X_input, perturbed_img)\n        \n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n\n    epsilon = 1e-6  # A small constant\n    weights = np.sqrt(np.exp(-(CVM_dist**2)/kernel_width**2)) + epsilon\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff_CVM = simpler_model.coef_[0]\n        \n    return coeff_CVM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff_cvm = xwhy_image_cvm(Xi_b, model=inceptionV3_model)\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_cvm)[-num_top_features:] \nmask = np.zeros(num_superpixels_b) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_b) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nheat_mask_cvm = create_heat_mask(superpixels_b, coeff_cvm)\n\nplt.imshow(heat_mask_cvm, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - CVMD')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def xwhy_image(X_input, model, num_perturb = 200, kernel_width = 0.25, top_pred_classes = top_pred_classes_b):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=5, max_dist=200, ratio=0.2)\n#     superpixels = skimage.segmentation.slic(X_input, n_segments=150, compactness=10, sigma=1,start_label=1)                 \n    \n    num_superpixels = np.unique(superpixels).shape[0]\n    perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n\n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled \n    distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n    \n    weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = LinearRegression()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)\n    coeff = simpler_model.coef_[0]\n        \n    return coeff, superpixels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff_lime, superpixels_lime = xwhy_image(Xi_b, model=inceptionV3_model)\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_lime)[-num_top_features:] \nmask = np.zeros(np.unique(superpixels_lime).shape[0]) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_lime) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_lime = create_heat_mask(superpixels_lime, coeff_lime)\n\nplt.imshow(heat_mask_lime, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - Cosine Distance')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"xwhy_all\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>XWhy All in One</b></div>","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 3, figsize=(30, 15)) \n\nfz = 20\n\nim = axs[0,0].imshow(heat_mask_lime, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[0,0])\ncbar.ax.tick_params(labelsize=14)  \naxs[0,0].set_title('Heatmap of LIME Coeffs - Cosine Distance', fontsize= fz)\naxs[0,0].axis('off')\n\nim = axs[0,1].imshow(heat_mask_k, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[0,1])\ncbar.ax.tick_params(labelsize=14)  \naxs[0,1].set_title('Heatmap of SMILE Coeffs - Kuiper Distance', fontsize= fz)\naxs[0,1].axis('off')\n\nim = axs[0,2].imshow(heat_mask_ks, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[0,2])\ncbar.ax.tick_params(labelsize=14)  \naxs[0,2].set_title('Heatmap of SMILE Coeffs - KSD', fontsize= fz)\naxs[0,2].axis('off')\n\nim = axs[1,0].imshow(heat_mask_wd, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[1,0])\ncbar.ax.tick_params(labelsize=14)  \naxs[1,0].set_title('Heatmap of SMILE Coeffs - WD', fontsize= fz)\naxs[1,0].axis('off')\n\nim = axs[1,1].imshow(heat_mask_ad, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[1,1])\ncbar.ax.tick_params(labelsize=14)  \naxs[1,1].set_title('Heatmap of SMILE Coeffs - ADD', fontsize= fz)\naxs[1,1].axis('off')\n\nim = axs[1,2].imshow(heat_mask_cvm, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[1,2])\ncbar.ax.tick_params(labelsize=14)  \naxs[1,2].set_title('Heatmap of SMILE Coeffs - CVMD', fontsize= fz)\naxs[1,2].axis('off')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"comp_SHAP\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Comparing XWhy with Weak Implementation of SHAP</b></div>","metadata":{}},{"cell_type":"code","source":"superpixels_b = skimage.segmentation.slic(Xi_b, n_segments=17, compactness=10, sigma=1,start_label=1)     \nnum_superpixels_b = np.unique(superpixels_b).shape[0]\n\nprint(num_superpixels_b)\n\nskimage.io.imshow(skimage.segmentation.mark_boundaries(Xi_b/2+0.5, superpixels_b))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\nimport sklearn\n\nimport scipy.special\nimport itertools\n\n# SHAP Implementation from \n# https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Simple%20Kernel%20SHAP.html\n\ndef perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n              mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n\ndef predict_proba_feature(X_input, perturbations):\n#     perturb_prediction = []\n#     for subset in perturbations:\n#         data = sample * subset\n#         perturb_prediction.append(np.max(model.predict(data)))\n    perturb_prediction = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        perturb_prediction.append(pred[0])\n\n    return perturb_prediction\n\ndef powerset(iterable):\n    s = list(iterable)\n    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1))\n\ndef shapley_kernel(M,s):\n    if s == 0 or s == M:\n        return 10000\n    return (M-1)/(scipy.special.binom(M,s)*s*(M-s))\n\ndef kernel_shap(f, x, reference, M, X_input):\n    X = np.zeros((2**M,M+1))\n    X[:,-1] = 1\n    weights = np.zeros(2**M)\n    V = np.zeros((2**M,M))\n    for i in range(2**M):\n        V[i,:] = reference\n\n    for i,s in enumerate(powerset(range(M))):\n        s = list(s)\n        V[i,s] = x[s]\n        X[i,s] = 1\n        weights[i] = shapley_kernel(M,len(s))\n    y = f(X_input, V)\n    tmp = np.linalg.inv(np.dot(np.dot(X.T, np.diag(weights)), X))\n    return np.dot(tmp, np.dot(np.dot(X.T, np.diag(weights)), y)) # Linear regression with kernel (weights)!\n\n\nsuperpixels = skimage.segmentation.slic(Xi_b, n_segments=21, compactness=7, sigma=1,start_label=1)               \nnum_superpixels = np.unique(superpixels).shape[0]\nperturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n\nM = num_superpixels\nreference = np.zeros(M)\nphi = kernel_shap(predict_proba_feature, np.ones(M), reference, M, Xi_b)\nshap_values = phi[:-1]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nSHAP_df = pd.DataFrame(shap_values.T)\nSHAP_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_shap_values = np.max(shap_values, axis=1)\nmax_shap_values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"average_shap_values = np.mean(shap_values, axis=1)\naverage_shap_values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_top_features = 1\ntop_features = np.argsort(max_shap_values)[-num_top_features:] \nmask = np.zeros(np.unique(superpixels).shape[0]) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_shap = create_heat_mask(superpixels, max_shap_values)\n\nplt.imshow(heat_mask_shap, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SHAP Max. Values')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_top_features = 1\ntop_features = np.argsort(average_shap_values)[-num_top_features:] \nmask = np.zeros(np.unique(superpixels).shape[0]) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_shap2 = create_heat_mask(superpixels, average_shap_values)\n\nplt.imshow(heat_mask_shap2, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SHAP Avg. Values')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"comp_BayLIME_Weak\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Comparing XWhy with Weak Implementation of BayLIME</b></div>","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import BayesianRidge\n\ndef xwhy_BayLIME_image(X_input, model, num_perturb = 200, kernel_width = 0.25, top_pred_classes = top_pred_classes_b):\n    \n    superpixels = skimage.segmentation.quickshift(X_input, kernel_size=5, max_dist=200, ratio=0.2)\n    \n    num_superpixels = np.unique(superpixels).shape[0]\n    perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))\n    \n    def perturb_image(img,perturbation,segments):\n        active_pixels = np.where(perturbation == 1)[0]\n        mask = np.zeros(segments.shape)\n        for active in active_pixels:\n            mask[segments == active] = 1 \n        perturbed_image = copy.deepcopy(img)\n        perturbed_image = perturbed_image*mask[:,:,np.newaxis]\n        return perturbed_image\n    \n    predictions = []\n    for pert in perturbations:\n        perturbed_img = perturb_image(X_input,pert,superpixels)\n        pred = inceptionV3_model.predict(perturbed_img[np.newaxis,:,:,:])\n        predictions.append(pred)\n\n    predictions = np.array(predictions)\n    \n    original_image = np.ones(num_superpixels)[np.newaxis,:] \n    distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()\n    \n    weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2))\n    \n    class_to_explain = top_pred_classes[0]\n    simpler_model = BayesianRidge()\n    simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain].ravel(), sample_weight=weights)\n    coeff = simpler_model.coef_\n        \n    return coeff, superpixels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\ncoeff_BayLIME, superpixels_BayLIME = xwhy_BayLIME_image(Xi_b, model=inceptionV3_model)\n\nnum_top_features = 1\ntop_features = np.argsort(coeff_BayLIME)[-num_top_features:] \nmask = np.zeros(np.unique(superpixels_BayLIME).shape[0]) \nmask[top_features]= True #Activate top superpixels\nskimage.io.imshow(perturb_image(Xi_b/2+0.5,mask,superpixels_BayLIME) )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heat_mask_BayLIME = create_heat_mask(superpixels_BayLIME, coeff_BayLIME)\n\nplt.imshow(heat_mask_BayLIME, cmap='plasma', interpolation='nearest')\nplt.colorbar()\nplt.title('Heatmap of SMILE Coeffs - Cosine Distance')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(3, 3, figsize=(30, 30)) \n\nfz = 20\n\nim = axs[0,0].imshow(heat_mask_lime, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[0,0])\ncbar.ax.tick_params(labelsize=14)  \naxs[0,0].set_title('Heatmap of LIME Coeffs - Cosine Distance', fontsize= fz)\naxs[0,0].axis('off')\n\nim = axs[0,1].imshow(heat_mask_k, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[0,1])\ncbar.ax.tick_params(labelsize=14)  \naxs[0,1].set_title('Heatmap of SMILE Coeffs - Kuiper Distance', fontsize= fz)\naxs[0,1].axis('off')\n\nim = axs[0,2].imshow(heat_mask_ks, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[0,2])\ncbar.ax.tick_params(labelsize=14)  \naxs[0,2].set_title('Heatmap of SMILE Coeffs - KSD', fontsize= fz)\naxs[0,2].axis('off')\n\nim = axs[1,0].imshow(heat_mask_wd, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[1,0])\ncbar.ax.tick_params(labelsize=14)  \naxs[1,0].set_title('Heatmap of SMILE Coeffs - WD', fontsize= fz)\naxs[1,0].axis('off')\n\nim = axs[1,1].imshow(heat_mask_ad, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[1,1])\ncbar.ax.tick_params(labelsize=14)  \naxs[1,1].set_title('Heatmap of SMILE Coeffs - ADD', fontsize= fz)\naxs[1,1].axis('off')\n\nim = axs[1,2].imshow(heat_mask_cvm, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[1,2])\ncbar.ax.tick_params(labelsize=14)  \naxs[1,2].set_title('Heatmap of SMILE Coeffs - CVMD', fontsize= fz)\naxs[1,2].axis('off')\n\nim = axs[2,0].imshow(heat_mask_BayLIME, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[2,0])\ncbar.ax.tick_params(labelsize=14)  \naxs[2,0].set_title('Heatmap of BayLIME Coeffs', fontsize= fz)\naxs[2,0].axis('off')\n\nim = axs[2,1].imshow(heat_mask_shap2, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[2,1])\ncbar.ax.tick_params(labelsize=14)  \naxs[2,1].set_title('Heatmap of SHAP Avg. Values', fontsize= fz)\naxs[2,1].axis('off')\n\nim = axs[2,2].imshow(heat_mask_shap, cmap='plasma', interpolation='nearest')\ncbar = fig.colorbar(im, ax=axs[2,2])\ncbar.ax.tick_params(labelsize=14)  \naxs[2,2].set_title('Heatmap of SHAP Max Values', fontsize= fz)\naxs[2,2].axis('off')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.savefig('my_figure.png', dpi=300, bbox_inches='tight')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"comp_SHAP\"></a>\n# <div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>Comparing XWhy with SHAP</b></div>","metadata":{}},{"cell_type":"markdown","source":"Under construction....","metadata":{}},{"cell_type":"code","source":"# # Source: https://github.com/gradient-ai/interpretable-ml-keras/blob/main/Interpreting%20Computer%20Vision%20models.ipynb\n# from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n# from keras.preprocessing import image\n# import shap\n# import json \n\n# # load pretrained model\n# model = ResNet50(weights='imagenet')\n# def predict(x):\n#     tmp = x.copy()\n#     preprocess_input(tmp)\n#     return model(tmp)\n\n# # get imagenet class names\n# url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n# with open(shap.datasets.cache(url)) as file:\n#     class_names = [v[1] for v in json.load(file).values()]\n    \n# def load_img(path):\n#     img = image.load_img(path, target_size=(224, 224))\n#     img = image.img_to_array(img)\n#     img = np.expand_dims(img, axis=0)\n#     return img\n\n# img = load_img('/kaggle/input/baylimeimgex5/5.jpg')\n    \n# # define a masker that is used to mask out partitions of the input image.\n# masker = shap.maskers.Image(\"inpaint_telea\", img.shape[1:])\n\n# # create an explainer with model and image masker\n# explainer = shap.Explainer(predict, masker, output_names=class_names,algorithm='partition')\n\n# # here we explain the same image and use 1000 evaluations of Resnet50 to get the shap values \n# shap_values = explainer(img, max_evals=1000, batch_size=50, outputs=shap.Explanation.argsort.flip[:4])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# shap_values.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# shap.image_plot(shap_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id = \"ref\"></a>\n<div style=\"padding:20px;color:white;margin:0;font-size:20px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b>References</b></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color:aliceblue; padding:30px; font-size:15px;color:#034914\">\n    \n* [[Interpretable Machine Learning with LIME for Image Classification]](https://nbviewer.org/url/arteagac.github.io/blog/lime_image.ipynb) by Cristian Arteaga used for Step-by-step implementation of LIME and then the rest of the code was added as the contribution of [X-Why](https://github.com/Dependable-Intelligent-Systems-Lab/xwhy).\n* [[LIME Paper:]](https://arxiv.org/abs/1602.04938) Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). \" Why should i trust you?\" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). [[LIME GitHub Repo]](https://github.com/marcotcr/lime).\n* [[BayLIME Paper:]](https://proceedings.mlr.press/v161/zhao21a/zhao21a.pdf) Zhao, X., Huang, W., Huang, X., Robu, V., and Flynn, D. (2021, December). Baylime: Bayesian local interpretable model-agnostic explanations. In Uncertainty in artificial intelligence (pp. 887-896). PMLR. [[BayLIME GitHub Repo]](https://github.com/x-y-zhao/BayLime).\n* [[SafeML Paper 1:]](https://arxiv.org/pdf/2005.13166.pdf) Aslansefat, K., Sorokos, I., Whiting, D., Tavakoli Kolagari, R., & Papadopoulos, Y. (2020). SafeML: safety monitoring of machine learning classifiers through statistical difference measures. In Model-Based Safety and Assessment: 7th International Symposium, IMBSA 2020, Lisbon, Portugal, September 1416, 2020, Proceedings 7 (pp. 197-211). Springer International Publishing. [[SafeML GitHub Repo]](https://github.com/ISorokos/SafeML).\n* [[SafeML Paper 2:]](https://bradscholars.brad.ac.uk/bitstream/handle/10454/18591/Abdullatif_ARA_2021.pdf?sequence=5) Aslansefat, K., Kabir, S., Abdullatif, A., Vasudevan, V., & Papadopoulos, Y. (2021). Toward improving confidence in autonomous vehicle software: A study on traffic sign recognition systems. Computer, 54(8), 66-76. [[SafeML GitHub Repo]](https://github.com/ISorokos/SafeML).   \n* [ EDA & ML on Game Play  (ongoing)](https://www.kaggle.com/code/nguyenthicamlai/eda-ml-on-game-play-ongoing) by [Nguyen Thi Cam Lai](https://www.kaggle.com/nguyenthicamlai) used for HTML-based headers","metadata":{}},{"cell_type":"markdown","source":"<center> <a href=\"#TOC\" role=\"button\" aria-pressed=\"true\" > Back to Table of Contents </a>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px;border:#034914 solid;padding: 15px;background-color:aliceblue;font-size:90%;text-align:left\">\n\n<h4><b>Author :</b> Koorosh Aslansefat </h4>\n\n<h4> <b>Some information:</b> </h4>\n\n<b>Check my Kaggle Notebooks :</b> https://www.kaggle.com/kooaslansefat <br>\n<b>Contact Me :</b> <a href=\"mailto:koo.ec2008@gmail.com\">koo.ec2008@gmail.com</a><br>\n<b>Find me LinkedIn :</b> www.linkedin.com/in/koorosh-aslansefat <br>\n<b>Find me Github :</b> https://github.com/koo-ec <br>\n    \n    \n<center> <strong> If you liked this Notebook, please do upvote. </strong>\n    \n<center> <strong> If you have any questions, feel free to contact me! </strong>\n    \n<center> <strong> Best Wishes </strong>","metadata":{}},{"cell_type":"markdown","source":"<center> <img src=\"https://gregcfuzion.files.wordpress.com/2022/01/kind-regards-2.png\" style='width: 600px; height: 300px;'>","metadata":{}}]}