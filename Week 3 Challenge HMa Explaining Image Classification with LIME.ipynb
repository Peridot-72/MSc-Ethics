{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhouiaLuqMuf"
   },
   "source": [
    "# **Week 3: Challenge Activity Part 1**\n",
    "In this lab work, we will applying LIME to explain black-box image classifiers.\n",
    "<br>\n",
    "<br>\n",
    "**LIME**<br>\n",
    "Generally, deep learning models are much more efficient than conventional ML models on image data as these models have the ability to perform auto feature extraction. They can extract complex low-level features such as stripes, edges, contours, corners and motifs, and even higher-level features such as larger shapes and certain parts of the object. These higher-level features are usually referred to as Regions of Interest (RoI) in the image, or superpixels, as they are collections of pixels of the image that cover a particular area of the image. Now, the low-level features are not human-interpretable, but the high-level features are human-interpretable, as any non-technical end user will relate to the images with respect to the higher-level features. LIME also works in a similar fashion. The algorithm tries to highlight the superpixels in images that contribute positively or negatively to the model's decision-making process hence providing explainability. Further information : [LIME GitHub](https://github.com/marcotcr/lime)\n",
    "<br>\n",
    "<br>\n",
    "**Please download shark image file from [Task 3 in Challenge Activity](https://canvas.hull.ac.uk/courses/75587/discussion_topics/481340?module_item_id=1375372) and save your Gdrive.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IZBsw4Tqro2",
    "outputId": "2e3408c2-fcf9-482a-bf8b-6cd9fda79828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from lime) (3.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from lime) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from lime) (1.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from lime) (4.66.5)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from lime) (1.7.0)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from lime) (0.24.0)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (3.3)\n",
      "Requirement already satisfied: pillow>=9.1 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (10.4.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from matplotlib->lime) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from matplotlib->lime) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from matplotlib->lime) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from matplotlib->lime) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from matplotlib->lime) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from matplotlib->lime) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from tqdm->lime) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hlmas\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the following libraries your local environment, if not already installed.\n",
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6UrP0yj0qHC1"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libararies\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as c_map\n",
    "from IPython.display import Image, display\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.xception import Xception, preprocess_input, decode_predictions\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from lime import submodular_pick\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkMWpRFAu6R7"
   },
   "source": [
    "## **Loading the data**\n",
    "Since we are more interested to check how black-box image classifiers can be explained using LIME, we will focus only on the inference part. Let us load any generic image data. For this example, we will take the data from this source: https://i.imgur.com/1Phh6hv.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "giXb5u-js_Uc",
    "outputId": "986625b3-ceb2-479a-bb95-92fb54c2b809"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Connect to your Google Drive\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      3\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/gdrive\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# Connect to your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "9bLjpItatOqX",
    "outputId": "d123cff6-4d74-4983-e1f6-65988e3f7be9"
   },
   "outputs": [],
   "source": [
    "image_path = \"c:/users/hlmas/Desktop/MSc/Ethics/MSc-Ethics/shark.jpg\" # Please change your filepath you saved the data.\n",
    "IMG_SIZE = (299, 299)\n",
    "\n",
    "# Display the image.\n",
    "display(Image(filename=image_path, width=IMG_SIZE[0], height=IMG_SIZE[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl5HCMVWrK2I"
   },
   "outputs": [],
   "source": [
    "def transform_image(image_path, size):\n",
    "    '''\n",
    "    Function to transform an image to normalized numpy array\n",
    "    '''\n",
    "    img = image.load_img(image_path, target_size=size)\n",
    "    img = image.img_to_array(img)# Transforming the image to get the shape as [channel, height, width]\n",
    "    img = np.expand_dims(img, axis=0) # Adding dimension to convert array into a batch of size (1,299,299,3)\n",
    "    img = img/255.0 # normalizing the image to keep within the range of 0.0 to 1.0\n",
    "\n",
    "    return img\n",
    "\n",
    "normalized_img = transform_image(image_path, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsI7QOFWrPeN"
   },
   "source": [
    "## **Defining the model**\n",
    "For this example, we are not training a model from scratch, but rather defining a pretrained Tensorflow Xception model as our black-box Deep Learning model which we will be explaining using the LIME framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOp172ARrP0f",
    "outputId": "f1d79738-42ca-47bb-ffb2-de29e3a547e6"
   },
   "outputs": [],
   "source": [
    "# Define and initialise a model.\n",
    "model = Xception(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "VXX40igkrRci",
    "outputId": "55a40dc5-7940-4045-8e10-08e198c6aaaa"
   },
   "outputs": [],
   "source": [
    "# Predict class of the image\n",
    "def get_model_predictions(data):\n",
    "    model_prediction = model.predict(data)\n",
    "    print(f\"The predicted class is : {decode_predictions(model_prediction, top=1)[0][0][1]}\")\n",
    "    return decode_predictions(model_prediction, top=1)[0][0][1] # get class name\n",
    "\n",
    "plt.imshow(normalized_img[0])\n",
    "pred_orig = get_model_predictions(normalized_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HoRA5dErVDC"
   },
   "source": [
    "The image is predicted as Tiger Shark which is the correct prediction and the black-box model is successfully able to give the correct prediction. Now, let us even take a look at the top 5 predictions along with the model confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uxJFX2LMrVRl",
    "outputId": "301a7c1b-2fa6-4361-ea7e-880e1fd5185e"
   },
   "outputs": [],
   "source": [
    "model_prediction = model.predict(normalized_img)\n",
    "top5_pred = decode_predictions(model_prediction, top=5)[0]\n",
    "for pred in top5_pred:\n",
    "    print(pred[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12lS7VOOrZot"
   },
   "source": [
    "As we see, although the model is well trained to produce the correct prediction, but there are chances that the model is not just looking into main object in the image but as well as the surrounding background. This is evident from the prediction of scuba_driver present in the top 5 prediction list. So, it is important for us understand, the key components or parts of the image the model is looking into to make the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lCX06S0rcv6"
   },
   "source": [
    "# **Model Explanation with LIME**\n",
    "Now, we will use the LIME framework to identify \"super-pixels\" or image segments used by the model to predict the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qyqwOz0ArZ46"
   },
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fc12ecb43a8b430a87467274398ee19a",
      "d999156476c2434b93c534d54d36e8a3",
      "d2b4c8191cfe423e8bdd96a4ad439152",
      "2f2270b83be542b683732a3a307f5178",
      "b199b542d7d445fba9145fc78fb60921",
      "b1f7fda061a042adbb4ae6d59022b896",
      "7d61c68cd2e64a8e804accdb775b3c40",
      "a85e6adae74c438a9476652bcec1c887",
      "cdad3df0a16d463db19da53ccdaa371c",
      "72a296e100cc4add8d51d3a5be644b66",
      "81620d1868bf401d99c1eda4ba29f635"
     ]
    },
    "id": "kKOGuatbreWu",
    "outputId": "307a5088-a07e-4c8f-b4cf-9186630ab46d"
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(normalized_img[0],\n",
    "                                 model.predict,\n",
    "                                 top_labels=5,\n",
    "                                 hide_color=0, # color code to mask\n",
    "                                 num_samples=1000) # number of pertutated images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08c7nMeqrh42"
   },
   "source": [
    "Our explainer object is ready, but let us visualize the various explanation segments created by the LIME algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "bCew1GPKrfsJ",
    "outputId": "4709f353-c01e-43f0-a19c-385b27d2640e"
   },
   "outputs": [],
   "source": [
    "plt.imshow(exp.segments)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eeVcSNcgrmIY"
   },
   "source": [
    "Now, let us use the top segments or super pixels to identify the region of interest of the image used by the model to make its prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZCkK5Izrj6I"
   },
   "outputs": [],
   "source": [
    "def generate_prediction_sample(exp, exp_class, weight = 0.1, show_positive = True, hide_background = True):\n",
    "    '''\n",
    "    Method to display and highlight super-pixels used by the black-box model to make predictions\n",
    "    '''\n",
    "    image, mask = exp.get_image_and_mask(exp_class,\n",
    "                                         positive_only=show_positive,\n",
    "                                         num_features=6, # number of super-pixels\n",
    "                                         hide_rest=hide_background,\n",
    "                                         min_weight=weight\n",
    "                                        )\n",
    "    plt.imshow(mark_boundaries(image, mask))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "tIXBOKvCrpXk",
    "outputId": "7736ca5c-f6f1-490d-c57e-81d121b8e285"
   },
   "outputs": [],
   "source": [
    "generate_prediction_sample(exp, exp.top_labels[0], show_positive = True, hide_background = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "UVvXlPhGrq5L",
    "outputId": "111341e7-1942-4707-a8c8-2997bb8e4ce3"
   },
   "outputs": [],
   "source": [
    "generate_prediction_sample(exp, exp.top_labels[0], show_positive = True, hide_background = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR9qMcHZrsu8"
   },
   "source": [
    "As we can see from the above image that the model was able to identify the correct region, which does indicate the correct prediction of the outcome by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "JMSNIiUvrvV8",
    "outputId": "452406cc-42e0-403f-e0e1-d23dc7ad55ef"
   },
   "outputs": [],
   "source": [
    "generate_prediction_sample(exp, exp.top_labels[0], show_positive = False, hide_background = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTc8QC8-rxFw"
   },
   "source": [
    "The above samples show us how we can hide or show the background along with the super-pixels or even outline or highlight the super-pixels to identify the region of interest used by the model to make the prediction. What we see from here does make sense, and does allow us to increase trust towards black-box models. We can also form a heat-map to show how important each super-pixel is to get some more granular explaianbility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "-NsXthVBryrP",
    "outputId": "b9f2e214-c813-4ec9-b672-b9bd8d77426e"
   },
   "outputs": [],
   "source": [
    "def explanation_heatmap(exp, exp_class):\n",
    "    '''\n",
    "    Using heat-map to highlight the importance of each super-pixel for the model prediction\n",
    "    '''\n",
    "    dict_heatmap = dict(exp.local_exp[exp_class])\n",
    "    heatmap = np.vectorize(dict_heatmap.get)(exp.segments)\n",
    "    plt.imshow(heatmap, cmap = 'RdBu', vmin  = -heatmap.max(), vmax = heatmap.max())\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "explanation_heatmap(exp, exp.top_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTTVqzSXr2T2"
   },
   "source": [
    "We can clearly identify the most influential segments used by the model to make the prediction using this heatmap visualization. Now, let try to perform the same steps for another explanation class and see if the results are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tuN988eWr5zb",
    "outputId": "8326a456-c3f4-4ca4-9de6-9b80b0eba8a1"
   },
   "outputs": [],
   "source": [
    "index = 2\n",
    "print(f\"We will deal with predicted class: {top5_pred[index][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38n289yJr7og"
   },
   "source": [
    "We will deal with predicted class: hammerhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "VGWWnhF8r9Ku",
    "outputId": "8083182c-9ae4-4c8e-d96c-0e6816ed12b6"
   },
   "outputs": [],
   "source": [
    "generate_prediction_sample(exp, exp.top_labels[index], weight = 0.0001, show_positive = False, hide_background = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "cZ-xUvS8r-qO",
    "outputId": "53c6fd97-753b-45a9-812f-712b7ac776e3"
   },
   "outputs": [],
   "source": [
    "explanation_heatmap(exp, exp.top_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg228hlZsAjR"
   },
   "source": [
    "In this case, we are trying to find out what made the model predict the outcome as hammerhead shark. When we used the LIME explaianbility methods, the visualizations clearly show that the middle part of the shark along with its fin, does contribute positively towards predicting the outcome as hammerhead shark, but the face and the front part contribute negatively towards the prediction. This is quite consistent with our human knowledge as well. Hammerhead Sharks are also sharks, so the middle part and the fin looks similar to Tiger Sharks but the face or the front portion of the hammerhead sharks looks like the shape of a hammer, which is significantly different from that of a tiger shark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oyf1O2-RsCmp"
   },
   "source": [
    "# **Task**\n",
    "Try with pretrained Tensorflow models (ResNet50, VGG16, VGG19, InceptionV3 and others), observe and analyse different outcomes.Refer: https://keras.io/api/applications/ to see differnet pretrained Tensorflow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvct7VoKsEl8"
   },
   "source": [
    "#### Using ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = image_path\n",
    "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(img_array, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class of the image\n",
    "def get_model_predictions(model, data):\n",
    "    model_prediction = model.predict(data)\n",
    "    print(f\"The predicted class is : {decode_predictions(model_prediction, top=1)[0][0][1]}\")\n",
    "    return decode_predictions(model_prediction, top=1)[0][0][1] # get class name\n",
    "\n",
    "plt.imshow(img)\n",
    "pred_orig = get_model_predictions(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = model.predict(x)\n",
    "top5_pred = decode_predictions(model_prediction, top=5)[0]\n",
    "for pred in top5_pred:\n",
    "    print(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(img_array.astype('double'),\n",
    "                                 model.predict,\n",
    "                                 top_labels=5,\n",
    "                                 hide_color=0, # color code to mask\n",
    "                                 num_samples=1000) # number of pertutated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(exp.segments)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prediction_sample(exp, exp.top_labels[0], weight=0.04, show_positive = True, hide_background = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_heatmap(exp, exp.top_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "print(f\"We will deal with predicted class: {top5_pred[index][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_prediction_sample(exp, exp.top_labels[index], weight = 0.02, show_positive = False, hide_background = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_heatmap(exp, exp.top_labels[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "img = keras.utils.load_img(img_path, target_size=(224, 224))\n",
    "img_array = keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2f2270b83be542b683732a3a307f5178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72a296e100cc4add8d51d3a5be644b66",
      "placeholder": "​",
      "style": "IPY_MODEL_81620d1868bf401d99c1eda4ba29f635",
      "value": " 1000/1000 [08:29&lt;00:00,  2.00it/s]"
     }
    },
    "72a296e100cc4add8d51d3a5be644b66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d61c68cd2e64a8e804accdb775b3c40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "81620d1868bf401d99c1eda4ba29f635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a85e6adae74c438a9476652bcec1c887": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b199b542d7d445fba9145fc78fb60921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1f7fda061a042adbb4ae6d59022b896": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdad3df0a16d463db19da53ccdaa371c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d2b4c8191cfe423e8bdd96a4ad439152": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a85e6adae74c438a9476652bcec1c887",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cdad3df0a16d463db19da53ccdaa371c",
      "value": 1000
     }
    },
    "d999156476c2434b93c534d54d36e8a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1f7fda061a042adbb4ae6d59022b896",
      "placeholder": "​",
      "style": "IPY_MODEL_7d61c68cd2e64a8e804accdb775b3c40",
      "value": "100%"
     }
    },
    "fc12ecb43a8b430a87467274398ee19a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d999156476c2434b93c534d54d36e8a3",
       "IPY_MODEL_d2b4c8191cfe423e8bdd96a4ad439152",
       "IPY_MODEL_2f2270b83be542b683732a3a307f5178"
      ],
      "layout": "IPY_MODEL_b199b542d7d445fba9145fc78fb60921"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
